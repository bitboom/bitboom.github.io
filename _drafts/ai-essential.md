# 인공지능 교육 과정 복습 자료

## 목차

1. [인공지능(AI) 기술 단계](#1-인공지능ai-기술-단계)
2. [인공지능 개요 및 목표](#2-인공지능-개요-및-목표)
3. [머신러닝(Machine Learning)](#3-머신러닝machine-learning)
4. [머신러닝을 사용하는 이유](#4-머신러닝을-사용하는-이유)
5. [머신러닝 학습 절차](#5-머신러닝-학습-절차)
6. [좋은 머신러닝 모델의 특성](#6-좋은-머신러닝-모델의-특성)
7. [머신러닝 학습 유형](#7-머신러닝-학습-유형)
8. [딥러닝(Deep Learning)](#8-딥러닝deep-learning)
9. [전통적 머신러닝과 딥러닝 비교](#9-전통적-머신러닝과-딥러닝-비교)
10. [인공신경망 개요](#10-인공신경망-개요)
11. [신경망의 기본 구성 요소](#11-신경망의-기본-구성-요소)
12. [순전파(Forward Propagation)](#12-순전파forward-propagation)
13. [활성화 함수 (Activation Function)](#13-활성화-함수-activation-function)
14. [활성화 함수 - Sigmoid](#14-활성화-함수---sigmoid)
15. [활성화 함수 - ReLU](#15-활성화-함수---relu)
16. [출력층 활성화 함수](#16-출력층-활성화-함수)
17. [순전파 과정 예시](#17-순전파-과정-예시)
18. [PyTorch에서 Tensor 데이터 선택](#18-pytorch에서-tensor-데이터-선택)
19. [손실 계산 (Loss Function)](#19-손실-계산-loss-function)
20. [평균 제곱 오차 (MSE)와 평균 절대 오차 (MAE)](#20-평균-제곱-오차-mse와-평균-절대-오차-mae)
21. [심층신경망 학습 과정 - 주요 개념 요약](#21-심층신경망-학습-과정---주요-개념-요약)
22. [합성곱 신경망 (CNN)](#22-합성곱-신경망-cnn)
23. [순환신경망 (RNN) 및 LSTM](#23-순환신경망-rnn-및-lstm)
24. [Seq2Seq 모델과 어텐션 메커니즘](#24-seq2seq-모델과-어텐션-메커니즘)
25. [RAG (Retrieval-Augmented Generation)](#25-rag-retrieval-augmented-generation)
26. [LangChain과 관련 개념](#26-langchain과-관련-개념)
27. [실습 코드 예제](#27-실습-코드-예제)
28. [요약 정리](#28-요약-정리)
29. [마지막 복습 포인트](#29-마지막-복습-포인트)
30. [복습을 위한 체크리스트](#30-복습을-위한-체크리스트)

---

## 1. 인공지능(AI) 기술 단계

- **인공지능 (AI)**: 인간의 지능을 모방하여 데이터 분석, 학습, 예측, 자율 행동 등을 수행하는 기술.
- **머신러닝**: 데이터 기반으로 학습하고 패턴을 찾아 추론하는 AI의 하위 분야. **지도 학습**과 **비지도 학습**으로 구분.
- **딥러닝**: 인공신경망을 사용하여 계층적으로 데이터를 학습하고 복잡한 문제를 해결하는 머신러닝의 한 분야.
- **생성형 AI**: 사용자의 특정 요구에 따라 결과물을 생성하는 AI 기술.
  - **VAE (Variational Autoencoder)**: 데이터의 확률 분포를 학습하여 새로운 데이터를 생성.
  - **GAN (Generative Adversarial Network)**: 두 네트워크가 경쟁하여 현실적인 데이터를 생성.
  - **LLM (Large Language Model)**: 대규모 언어 모델을 사용하여 자연어 처리 및 생성 수행.

---

## 2. 인공지능 개요 및 목표

- **인공지능**: 인간의 지능적 행동을 모방하는 컴퓨터 시스템 또는 기계.
- **AI의 목표**: 입력 데이터와 출력 결과 간의 관계를 학습하여 문제를 해결.
- **AI의 분류**:
  - **좁은 인공지능 (ANI)**: 특정 작업에 특화된 AI.
  - **일반 인공지능 (AGI)**: 인간과 비슷한 수준의 지능을 가진 AI로, 다양한 작업 수행 가능.

---

## 3. 머신러닝(Machine Learning)

- **정의**: 명시적인 프로그래밍 없이 데이터로부터 학습하고 규칙을 도출하여 문제를 해결하는 기술.
- **학습 방법**:
  - **생성 모델**: 데이터의 분포를 학습하여 새로운 데이터 생성.
  - **판별 모델**: 입력 데이터를 기반으로 목표 변수를 직접 예측.

---

## 4. 머신러닝을 사용하는 이유

- 복잡하거나 명확한 규칙이 없는 문제를 데이터 기반으로 해결.
- 비정형 데이터의 복잡성과 불규칙성을 처리하여 패턴을 자동으로 학습.

---

## 5. 머신러닝 학습 절차

1. **문제 정의**: 해결하고자 하는 문제와 목표 설정.
2. **데이터 수집**: 학습에 필요한 데이터 확보.
3. **데이터 전처리**: 데이터 정제 및 변환.
4. **모델 선택**: 최적의 알고리즘과 모델 선정.
5. **모델 학습 및 평가**: 모델을 학습시키고 성능 평가.

---

## 6. 좋은 머신러닝 모델의 특성

- **성능**: 훈련 데이터와 미학습 데이터 모두에서 높은 성능.
- **예측 정확도**: 문제에 대한 정확한 예측 능력.
- **적절한 복잡성**: 데이터의 복잡도에 맞는 모델 복잡성 유지.

---

## 7. 머신러닝 학습 유형

- **지도 학습 (Supervised Learning)**: 레이블이 있는 데이터를 사용하여 학습.
- **비지도 학습 (Unsupervised Learning)**: 레이블이 없는 데이터를 사용하여 패턴 발견.
- **자기 지도 학습 (Self-supervised Learning)**: 데이터 자체에서 레이블을 생성하여 학습.

---

## 8. 딥러닝(Deep Learning)

- **정의**: 인공신경망을 기반으로 데이터를 학습하고 복잡한 패턴을 인식하는 기술.
- **특징**:
  - 대규모 데이터 처리 능력.
  - 전이 학습 능력.
  - 이미지 인식, 음성 인식, 자연어 처리에서 뛰어난 성능.
- **구조**: 여러 층의 뉴런으로 구성된 인공신경망 사용.

---

## 9. 전통적 머신러닝과 딥러닝 비교

| 구분           | 전통적 머신러닝                               | 딥러닝                                      |
| -------------- | --------------------------------------------- | ------------------------------------------- |
| **데이터 종류** | 소량의 정형 데이터                            | 대량의 비정형 데이터                        |
| **특징 추출**  | 수동으로 중요한 변수 추출                     | 자동으로 특징 추출                          |
| **구조**       | 간단한 알고리즘                               | 다층 신경망 활용한 복잡한 구조              |
| **컴퓨팅 자원** | 적은 자원 필요                                | 고성능 GPU 등 많은 자원 필요                |
| **응용 분야**  | 가격 예측, 고장 예측                          | 자율 주행, 이미지 인식 등 고차원 문제 해결  |

---

## 10. 인공신경망 개요

- **정의**: 인간의 뇌 구조를 모방한 모델로, 데이터 학습과 패턴 인식에 사용.
- **퍼셉트론(Perceptron)**:
  - **단층 퍼셉트론**: 선형 분류 가능.
  - **다층 퍼셉트론(MLP)**: 은닉층 추가로 비선형 문제 해결 가능.

### 실습: OR 문제를 위한 단층 퍼셉트론 구성

**연습문제-03-001**: OR 문제를 해결하기 위한 입력과 출력 데이터 생성

```python
import torch

# OR 문제에 사용할 입력과 출력 데이터 생성
X = torch.tensor([[0.0, 0.0],
                  [0.0, 1.0],
                  [1.0, 0.0],
                  [1.0, 1.0]])

y = torch.tensor([[0.0],
                  [1.0],
                  [1.0],
                  [1.0]])
```

- **설명**: OR 논리 게이트 문제를 해결하기 위한 입력 `X`와 출력 `y` 데이터를 생성합니다. 각 입력은 두 개의 이진 값으로 구성되며, 출력은 OR 연산의 결과입니다.

**연습문제-03-002**: OR 문제를 위한 단층 퍼셉트론 구성

```python
import torch.nn as nn

# 단층 퍼셉트론 구성
layer = nn.Linear(2, 1)  # 입력이 2개, 출력이 1개인 선형 레이어
model = nn.Sequential(
    layer,
    nn.Sigmoid()  # 활성화 함수로 시그모이드 사용
)
```

- **설명**: 입력이 2개이고 출력이 1개인 선형 레이어를 생성하고, 시그모이드 활성화 함수를 적용하여 단층 퍼셉트론을 구성합니다.

**연습문제-03-003**: OR 문제를 위한 단층 퍼셉트론 결과 확인

```python
# 모델 예측 결과 확인
with torch.no_grad():
    outputs = model(X)
    print(outputs)
```

- **설명**: 생성된 모델에 입력 `X`를 전달하여 예측 결과를 확인합니다.

**결과 예시**:

```
tensor([[0.5000],
        [0.7311],
        [0.7311],
        [0.8808]])
```

- 예측된 출력값이 OR 연산의 결과와 유사함을 확인할 수 있습니다.

---

## 11. 신경망의 기본 구성 요소

- **nn.Linear**: 입력에 선형 변환을 적용하는 완전 연결층.
  - **구성**: 입력 차원과 출력 차원을 정의하며, 가중치와 편향 포함.

---

## 12. 순전파(Forward Propagation)

- 입력 데이터를 받아 출력 결과를 계산하는 과정.
- 각 층을 통과하면서 가중치와 활성화 함수를 적용하여 출력 계산.

---

## 13. 활성화 함수 (Activation Function)

- **역할**: 신경망에 비선형성을 부여하여 복잡한 패턴 학습 가능.
- **종류**:
  - **ReLU**: 은닉층에서 주로 사용, 음수는 0으로 변환.
  - **Sigmoid**: 출력층에서 사용, 값을 0과 1 사이로 변환.
  - **기타**: Leaky ReLU, ELU 등.

---

## 14. 활성화 함수 - Sigmoid

- **공식**: σ(x) = 1 / (1 + e<sup>-x</sup>)
- **특징**:
  - 출력값을 0에서 1 사이로 변환하여 확률 해석 가능.
  - 기울기 소실 문제 발생 가능.
  - 중심이 0이 아니어서 학습이 느릴 수 있음.

---

## 15. 활성화 함수 - ReLU

- **공식**: ReLU(x) = max(0, x)
- **특징**:
  - 기울기 소실 문제 적어 학습이 빠름.
  - 음수 입력에 대해 0을 출력하여 뉴런이 죽을 수 있음.

---

## 16. 출력층 활성화 함수

- **회귀 문제**: 활성화 함수 사용 안 함.
- **이진 분류**: Sigmoid 함수 사용.
- **다중 클래스 분류**: Softmax 함수 사용하여 각 클래스 확률 계산.

---

## 17. 순전파 과정 예시

- **구조**: 3개의 입력 노드, 2개의 은닉 노드, 1개의 출력 노드.
- **과정**:
  - 입력 데이터를 가중치와 곱하고 활성화 함수를 적용하며 각 층을 통과.
  - 최종 출력층에서 활성화 함수를 적용하여 결과 도출.

---

## 18. PyTorch에서 Tensor 데이터 선택

- **인덱싱과 슬라이싱**:
  - Python 리스트와 유사하게 사용.
  - 다차원 텐서의 경우 `[행, 열]` 형태로 접근.
- **예시**:
  - `tensor_1d[1:4]`: 인덱스 1부터 3까지 선택.

---

## 19. 손실 계산 (Loss Function)

- **역할**: 모델의 예측 값과 실제 값 사이의 차이를 수치화.
- **유형**:
  - **회귀 문제**: MSE, MAE 사용.
  - **분류 문제**: 교차 엔트로피(Cross-Entropy) 사용.

---

## 20. 평균 제곱 오차 (MSE)와 평균 절대 오차 (MAE)

- **MSE**:
  - 예측 값과 실제 값의 차이를 제곱하여 평균.
  - 큰 오차에 더 큰 페널티.
- **MAE**:
  - 예측 값과 실제 값의 차이의 절대값 평균.
  - 오차 크기가 클 때 민감하지 않음.

---

## 21. 심층신경망 학습 과정 - 주요 개념 요약

### 1. 손실 함수 (Loss Function)

- **역할**: 예측값과 실제 값 간의 차이를 측정하여 학습 최적화에 활용.
- **종류**:
  - **교차 엔트로피(Cross Entropy)**: 다중 클래스 분류 문제에 효과적.
  - **이진 교차 엔트로피(Binary Cross-Entropy)**: 이진 분류 문제에 적용.

### 2. 역전파 (Backpropagation)

- **과정**: 손실 값을 기반으로 각 가중치가 손실에 미치는 영향 계산.
- **방법**: 체인 룰을 사용하여 미분을 연쇄적으로 계산.
- **목적**: 가중치를 업데이트하여 손실 최소화.

### 3. 경사 하강법 (Gradient Descent)

- **역할**: 손실 함수의 기울기를 이용하여 파라미터 최적화.
- **학습률(Learning Rate)**: 업데이트하는 스텝의 크기를 결정.

### 4. 기울기 소실 및 폭발 문제

- **문제점**:
  - **기울기 소실**: 기울기가 너무 작아져 학습이 정체.
  - **기울기 폭발**: 기울기가 너무 커져 불안정한 학습.
- **해결책**: 적절한 활성화 함수 선택, 기울기 클리핑 등.

### 5. 배치 경사 하강법 유형

- **배치 경사 하강법**: 전체 데이터셋 사용, 안정적이지만 느림.
- **확률적 경사 하강법 (SGD)**: 하나의 샘플로 업데이트, 빠르지만 불안정.
- **미니배치 경사 하강법**: 작은 배치로 업데이트, 속도와 안정성 균형.

### 6. 반복 학습 (Epoch)

- **에폭(Epoch)**: 전체 데이터셋이 한 번 모델을 통과한 상태.
- **조기 종료**: 과적합 방지를 위해 검증 성능이 향상되지 않으면 학습 중단.

### 7. 데이터셋 및 Fashion-MNIST

- **Fashion-MNIST**: 의류 이미지로 구성된 10개 클래스 데이터셋.
- **구성**: 28x28 픽셀 흑백 이미지, 60,000개 훈련 샘플, 10,000개 테스트 샘플.

### 8. PyTorch의 Dataset 클래스 커스터마이징

- **CustomDataset**: 데이터 로드와 전처리를 위한 사용자 정의 클래스.
- **DataLoader와 함께 사용**: 배치 처리 및 셔플링 지원.

#### 실습: 커스텀 데이터셋과 DataLoader 생성

**연습문제-03-004**: 150개의 샘플과 3개의 클래스 레이블을 가진 새로운 랜덤 데이터를 사용하여 커스텀 데이터셋을 생성하고, DataLoader를 설정합니다.

```python
import torch
from torch.utils.data import Dataset, DataLoader

# 새로운 랜덤 데이터 생성
data = torch.randn(150, 4)  # 150개의 샘플, 각 샘플은 4차원 벡터
labels = torch.randint(0, 3, (150,))  # 3개의 클래스로 분류되는 150개의 레이블

# 커스텀 데이터셋 클래스 정의
class CustomDataset(Dataset):
    def __init__(self, data, labels):
        self.data = data
        self.labels = labels
    def __len__(self):
        return len(self.data)
    def __getitem__(self, idx):
        return self.data[idx], self.labels[idx]

# 커스텀 데이터셋과 DataLoader 생성
dataset = CustomDataset(data, labels)

# 미니 배치 크기는 5
dataloader = DataLoader(dataset, batch_size=5)
```

- **설명**: `CustomDataset` 클래스를 정의하여 데이터와 레이블을 관리하고, `DataLoader`를 사용하여 배치 단위로 데이터를 로드합니다.

**연습문제-03-005**: DataLoader에서 배치 조회

```python
# DataLoader에서 모든 배치를 리스트로 변환
all_batches = list(dataloader)

# 첫 번째 배치 조회
batch_data, batch_labels = all_batches[0]  # 첫 번째 배치의 데이터와 레이블 조회
print(batch_data)
print(batch_labels)
```

- **설명**: DataLoader에서 첫 번째 배치를 가져와 데이터와 레이블을 확인합니다.

---

## 22. 합성곱 신경망 (CNN)

- **합성곱 층(Convolutional Layer)**: 이미지의 공간적 구조와 패턴을 추출.
- **풀링 층(Pooling Layer)**: 특징 맵의 크기를 줄여 계산 효율 향상 및 과적합 방지.
- **응용 분야**: 이미지 분류, 객체 탐지 등.

### 실습: CNN 모델 설계 (Sequential 방식)

**연습문제-05-001**: (배치, 3, 32, 32) 입력을 받는 CNN 모델 설계

```python
import torch
import torch.nn as nn

model = nn.Sequential(
    # 첫 번째 합성곱층과 풀링층
    nn.Conv2d(3, 16, kernel_size=3, padding=1),  # 입력 채널 3, 출력 채널 16
    nn.ReLU(),  # 활성화 함수
    nn.MaxPool2d(kernel_size=2, stride=2),  # 출력 크기 절반 감소

    # 두 번째 합성곱층과 풀링층
    nn.Conv2d(16, 32, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.MaxPool2d(kernel_size=2, stride=2),

    # 세 번째 합성곱층과 풀링층
    nn.Conv2d(32, 64, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.MaxPool2d(kernel_size=2, stride=2),

    # Flatten 레이어
    nn.Flatten(),

    # 완전 연결층
    nn.Linear(64 * 4 * 4, 128),
    nn.ReLU(),
    nn.Linear(128, 10)  # 10개의 클래스로 분류
)

print(model)
```

- **설명**: CIFAR-10과 같은 32x32 컬러 이미지를 입력으로 받아 10개의 클래스로 분류하는 CNN 모델을 설계합니다.

**연습문제-05-002**: (배치, 3, 224, 224) 입력을 받는 CNN 모델 설계

```python
import torch
import torch.nn as nn

class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()

        # 첫 번째 합성곱층과 풀링층
        self.conv1 = nn.Conv2d(3, 256, kernel_size=3, padding=1)
        self.relu1 = nn.ReLU()
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)

        # 두 번째 합성곱층과 풀링층
        self.conv2 = nn.Conv2d(256, 128, kernel_size=3, padding=1)
        self.relu2 = nn.ReLU()
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)

        # 세 번째 합성곱층과 풀링층
        self.conv3 = nn.Conv2d(128, 64, kernel_size=3, padding=1)
        self.relu3 = nn.ReLU()
        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)

        # 네 번째 합성곱층과 풀링층
        self.conv4 = nn.Conv2d(64, 32, kernel_size=3, padding=1)
        self.relu4 = nn.ReLU()
        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)

        # Flatten 레이어
        self.flatten = nn.Flatten()

        # 완전 연결층
        self.fc1 = nn.Linear(32 * 14 * 14, 256)
        self.relu_fc1 = nn.ReLU()
        self.fc2 = nn.Linear(256, 2)

    def forward(self, x):
        x = self.pool1(self.relu1(self.conv1(x)))
        x = self.pool2(self.relu2(self.conv2(x)))
        x = self.pool3(self.relu3(self.conv3(x)))
        x = self.pool4(self.relu4(self.conv4(x)))
        x = self.flatten(x)
        x = self.relu_fc1(self.fc1(x))
        x = self.fc2(x)
        return x

model = CNN()
print(model)
```

- **설명**: 224x224 크기의 이미지를 입력으로 받아 2개의 클래스로 분류하는 CNN 모델을 Module 방식으로 설계합니다.

---

## 23. 순환신경망 (RNN) 및 LSTM

- **RNN**: 순차적 데이터(시계열, 자연어 등) 처리에 특화된 신경망.
  - 이전의 출력이 현재 입력에 영향을 미침.
- **LSTM (Long Short-Term Memory)**:
  - 장기 의존성 문제 해결을 위한 게이트 구조 도입.
  - **게이트**: 입력 게이트, 망각 게이트, 출력 게이트로 구성되어 정보 흐름 제어.

### 실습: 문서 유형 분류를 위한 LSTM 모델 정의

**연습문제-06-001**: 10개 문서 유형 분류를 위한 모델 정의 (Module 방식)

```python
import torch
from torch import nn

# DocumentClassifier 모델 정의
class DocumentClassifier(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_size, num_class=10):
        super().__init__()

        # Embedding 계층
        self.embedding = nn.Embedding(vocab_size, embed_dim)

        # LSTM 계층
        self.lstm = nn.LSTM(embed_dim, hidden_size, batch_first=True)

        # 완전 연결층
        self.fc = nn.Linear(hidden_size, num_class)

    def forward(self, x):
        x = self.embedding(x)          # 임베딩
        _, (h_n, _) = self.lstm(x)     # LSTM 처리
        out = self.fc(h_n[-1])         # 최종 은닉 상태를 사용하여 출력 계산
        return out

# 하이퍼파라미터 설정
vocab_size = 5000   # 단어 사전 크기
embed_dim = 300     # 임베딩 차원
hidden_size = 128   # LSTM 은닉 상태 크기

# 모델 생성
model = DocumentClassifier(vocab_size, embed_dim, hidden_size)
print(model)
```

- **설명**: LSTM을 사용하여 문서의 순차적 정보를 학습하고, 완전 연결층을 통해 10개의 문서 유형으로 분류하는 모델을 정의합니다.

---

## 24. Seq2Seq 모델과 어텐션 메커니즘

- **Seq2Seq (Sequence-to-Sequence) 모델**:
  - **정의**: 입력 시퀀스를 출력 시퀀스로 변환하는 모델.
  - **구조**: 인코더와 디코더로 구성된 RNN 기반 모델.
  - **응용 분야**: 기계 번역, 요약 등.
- **어텐션 메커니즘 (Attention Mechanism)**:
  - **역할**: 인코더의 모든 출력에 가중치를 부여하여 중요한 정보에 집중.
  - **효과**: 장기 의존성 문제 완화 및 성능 향상.
  - **Self-Attention**: 입력 시퀀스 내의 각 요소가 다른 모든 요소와 관계를 학습.

---

## 25. RAG (Retrieval-Augmented Generation)

- **정의**: 검색된 정보를 기반으로 텍스트를 생성하는 모델.
- **구성 요소**:
  - **Retriever**: 관련 문서나 정보를 검색.
  - **Generator**: 검색된 정보를 바탕으로 응답 생성.
- **장점**: 최신 정보나 특정 도메인 지식을 활용하여 보다 정확한 응답 생성.

---

## 26. LangChain과 관련 개념

- **LangChain**: 대형 언어 모델(LLM)을 활용한 애플리케이션 개발 프레임워크.
- **LangChain Expression Language**:
  - LangChain에서 사용하는 표현 언어로, 컴포넌트 간의 상호 작용 정의.
- **Runnable 인터페이스**:
  - **구성 요소**:
    - **Prompt**: LLM에 입력될 프롬프트를 생성.
    - **ChatModel**: 대화형 LLM 모델.
    - **LLM**: 대형 언어 모델.
    - **OutputParser**: LLM의 출력을 원하는 형식으로 파싱.
    - **Retriever**: 정보 검색을 위한 모듈.
    - **Tool**: 외부 기능이나 API를 활용하기 위한 도구.
- **Agent**:
  - 다양한 도구와 상호 작용하여 복잡한 작업을 수행하는 컴포넌트.
  - **역할**: LLM이 툴을 사용하여 문제를 해결하도록 안내.
- **Output Parser**:
  - LLM의 출력을 구조화된 데이터로 변환.
- **Document Loader**:
  - 문서를 로드하여 처리 가능한 형식으로 변환.
- **Vector Store**:
  - 텍스트 데이터를 벡터로 변환하여 저장하고 유사도 검색에 활용.

---

## 27. 실습 코드 예제

### 1. OR 문제를 위한 단층 퍼셉트론

[위의 **10. 인공신경망 개요** 섹션 참고]

### 2. 커스텀 데이터셋과 DataLoader 생성

[위의 **21. 심층신경망 학습 과정 - 주요 개념 요약** 섹션의 **PyTorch의 Dataset 클래스 커스터마이징** 참고]

### 3. CNN 모델 설계

[위의 **22. 합성곱 신경망 (CNN)** 섹션 참고]

### 4. 문서 분류를 위한 LSTM 모델

[위의 **23. 순환신경망 (RNN) 및 LSTM** 섹션 참고]

---

## 28. 요약 정리

### 인공지능(AI) 기술 단계 및 개요

- **AI**: 인간의 지능을 모방하여 다양한 기능 수행.
- **머신러닝**: 데이터 기반 학습, 지도/비지도 학습.
- **딥러닝**: 인공신경망을 통한 계층적 데이터 학습.
- **생성형 AI**: 요구에 따른 결과물 생성 (VAE, GAN, LLM).

### 머신러닝 학습 절차 및 방법

1. **문제 정의**
2. **데이터 수집**
3. **데이터 전처리**
4. **모델 선택**
5. **모델 학습 및 평가**

- **지도 학습**: 레이블 있는 데이터로 학습.
- **비지도 학습**: 레이블 없이 패턴 탐색.
- **자기 지도 학습**: 데이터에서 레이블 생성하여 학습.

### 딥러닝 및 인공신경망

- **딥러닝**: 복잡한 패턴 인식에 강점.
- **퍼셉트론**: 단층은 선형 분류, 다층은 비선형 문제 해결.
- **활성화 함수**: ReLU, Sigmoid 등 비선형성 부여.

### 학습 과정과 손실 함수

- **순전파**: 입력 데이터를 통해 출력 계산.
- **역전파**: 손실에 따른 가중치 업데이트.
- **손실 함수**:
  - **회귀**: MSE 사용.
  - **분류**: Cross-Entropy 사용.

### 과적합 방지 및 성능 향상

- **방법**:
  - 데이터 증가
  - 정규화(L1/L2)
  - 드롭아웃
  - 조기 종료
- **배치 정규화**: 학습 속도와 안정성 향상.

### PyTorch를 활용한 모델 구성

- **nn.Sequential**: 순차적 모델 구성.
- **nn.Module**: 커스텀 신경망 구현 시 사용.
- **DataLoader**: 배치 단위 데이터 로드.

### 합성곱 신경망 (CNN)

- **합성곱 층**: 특징 추출.
- **풀링 층**: 특성 맵 크기 감소, 과적합 방지.
- **전이 학습**: 사전 학습된 모델 활용.

### 순환신경망 (RNN) 및 LSTM

- **RNN**: 순차 데이터 처리에 특화.
- **LSTM**: 장기 의존성 문제 해결.
- **Seq2Seq 모델**: 입력 시퀀스를 출력 시퀀스로 변환.
- **어텐션 메커니즘**: 중요한 정보에 집중하여 성능 향상.

### RAG와 LangChain

- **RAG**: 검색된 정보를 활용하여 텍스트 생성.
- **LangChain**: LLM 기반 애플리케이션 개발 프레임워크.
  - **Runnable 인터페이스**: 컴포넌트 모듈화 및 결합.
  - **Agent**: 복잡한 작업 수행을 위한 구성 요소.

---

## 29. 마지막 복습 포인트

- **인공지능 기술 단계**를 이해하고, **머신러닝**과 **딥러닝**의 차이점을 명확히 구분.
- **머신러닝 학습 절차**를 따라 모델 개발 프로세스를 숙지.
- **신경망의 구성 요소**와 **활성화 함수**의 역할을 이해하여 모델 구조 설계에 적용.
- **손실 함수**와 **최적화 알고리즘**의 선택이 모델 성능에 미치는 영향을 파악.
- **과적합 방지 기법**을 통해 모델의 일반화 성능을 높이는 방법을 학습.
- **PyTorch 프레임워크**를 활용하여 실제 딥러닝 모델을 구현하고 실험.
- **합성곱 신경망(CNN)**, **순환신경망(RNN)**, **LSTM**, **Seq2Seq 모델**, **어텐션 메커니즘**의 구조와 응용 분야를 이해.
- **RAG**와 **LangChain** 등 최신 기술의 개념과 적용 방법을 익힘.

---

## 30. 복습을 위한 체크리스트

- [ ] 인공지능과 머신러닝, 딥러닝의 개념과 차이점을 이해하였는가?
- [ ] 머신러닝의 학습 절차와 각 단계의 중요성을 숙지하였는가?
- [ ] 인공신경망의 구성 요소와 활성화 함수의 역할을 파악하였는가?
- [ ] 손실 함수의 종류와 적용 분야를 알고 있는가?
- [ ] 과적합 방지 방법과 모델의 일반화 성능 향상 기법을 이해하였는가?
- [ ] PyTorch를 활용한 모델 구현 방법을 실습하였는가?
- [ ] CNN과 RNN, LSTM, Seq2Seq 모델의 구조와 각각의 응용 분야를 구분할 수 있는가?
- [ ] 어텐션 메커니즘의 원리를 이해하고 활용할 수 있는가?
- [ ] RAG 모델의 구성과 장점을 이해하였는가?
- [ ] LangChain의 주요 구성 요소와 활용 방법을 숙지하였는가?

---

이 자료를 통해 인공지능의 기본 개념부터 딥러닝 모델의 심화 내용까지 체계적으로 복습하시기 바랍니다. 특히 최신 기술인 **RAG**, **LangChain**, **어텐션 메커니즘** 등의 개념을 이해하고 실무에 적용할 수 있도록 학습하시기 바랍니다.
